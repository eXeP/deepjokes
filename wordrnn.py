import torch.nn as nn
import numpy as np
import data
import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import torch.optim as optim
import random
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
import gc

device = torch.device('cuda:0')
print(torch.__version__)

class Encoder(nn.Module):
    def __init__(self, dictionary_size, hidden_size):
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(dictionary_size, hidden_size)
        self.gru = nn.LSTM(hidden_size, hidden_size)

    def forward(self, pad_seqs, seq_lengths, hidden):
        """
        Args:
          pad_seqs: Tensor [max_seq_length, batch_size, 1]
          seq_lengths: list of sequence lengths
          hidden: Tensor [1, batch_size, hidden_size]

        Returns:
          outputs: Tensor [max_seq_length, batch_size, hidden_size]
          hidden: Tensor [1, batch_size, hidden_size]
        """
        # YOUR CODE HERE
        x = self.embedding(pad_seqs)
        x = x.view(x.shape[0],x.shape[1], -1)
        x = pack_padded_sequence(x, seq_lengths)
        outputs, hidden = self.gru(x)
        outputs = pad_packed_sequence(outputs)
        return outputs[0], hidden

    def init_hidden(self, batch_size=1, device=device):
        return torch.zeros(1, batch_size, self.hidden_size, device=device)

class Decoder(nn.Module):
    def __init__(self, hidden_size, output_dictionary_size, SOS_token):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.SOS_token = SOS_token
        self.embedding = nn.Embedding(output_dictionary_size, hidden_size)
        self.gru = nn.LSTM(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_dictionary_size)

    def forward(self, hidden, pad_target_seqs=None, teacher_forcing=False):
        """
        Args:
          hidden (tensor):          The state of the GRU (shape [1, batch_size, hidden_size])
          pad_target_seqs (tensor): Tensor of words (word indices) of the target sentence. The shape is
                                     [max_seq_length, batch_size, 1]. If None, the output sequence
                                     is generated by feeding the decoder's outputs (teacher_forcing has to be False).

        Returns:
          outputs (tensor): Tensor of log-probabilities of words in the output language
                             (shape [max_seq_length, batch_size, output_dictionary_size])
          hidden (tensor):  New state of the GRU (shape [1, batch_size, hidden_size])
        """
        if pad_target_seqs is None:
            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'

        batch_size = hidden[0].size(1)
        prev_word = torch.tensor(self.SOS_token * np.ones((1, batch_size)), device=device, dtype=torch.int64)
        max_length = pad_target_seqs.size(0) if pad_target_seqs is not None else MAX_LENGTH
        outputs = []  # Collect outputs of the decoder at different steps in this list
        for t in range(max_length):
            # YOUR CODE HERE
            output = self.embedding(prev_word).view(1, batch_size, -1)
            output = F.relu(output)
            output, hidden = self.gru(output)
            
            output = self.out(output[0])
            output = F.log_softmax(output, dim=1)
            output = output.view(1, batch_size, -1)
            outputs.append(output)

            if teacher_forcing:
                # Feed the target as the next input
                prev_word = pad_target_seqs[t]
            else:
                # Use its own predictions as the next input
                topv, topi = output[0, :].topk(1)
                prev_word = topi.squeeze().detach()  # detach from history as input

        outputs = torch.cat(outputs, dim=0)  # [max_length, batch_size, output_dictionary_size]

        return outputs, hidden

    def init_hidden(self, batch_size, device=device):
        return torch.zeros(1, batch_size, self.hidden_size, device=device)
def collate(list_of_samples):
    """Merges a list of samples to form a mini-batch.

    Args:
      list_of_samples is a list of tuples (input_seq, output_seq),
      input_seq is Tensor([seq_length, 1])
      output_seq is Tensor([seq_length, 1])

    Returns:
      input_seqs: Tensor of padded input sequences: [max_seq_length, batch_size, 1].
      output_seqs: Tensor of padded output sequences: [max_seq_length, batch_size, 1].
    """

    # sort a list by sequence length (descending order) to use pack_padded_sequence
    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)

    input_seqs, output_seqs = zip(*list_of_samples)
    input_seq_lengths = [len(seq) for seq in input_seqs]
    output_seq_lengths = [len(seq) for seq in output_seqs]

    padding_value = 0
    
    # Put all input sequences to one tensor, pad with padding_value
    pad_input_seqs = pad_sequence(input_seqs, batch_first=False, padding_value=padding_value)
    
    # Put all output sequences to one tensor, pad with padding_value
    # We cannot use pad_sequence because the output sequences are not necessarily sorted according to the lengths
    # YOUR CODE HERE
    pad_output_seqs = pad_sequence(output_seqs, batch_first=False, padding_value=padding_value)
    return pad_input_seqs, input_seq_lengths, pad_output_seqs, output_seq_lengths

def compute_loss(decoder_outputs, pad_target_seqs, padding_value=0):

    n = 0
    loss = 0.0
    for i in range(0, decoder_outputs.shape[0]):
        mask = (pad_target_seqs[i] > padding_value).float()
        n = n+mask.sum()
        loss += (criterion(decoder_outputs[i], pad_target_seqs[i]) * mask).sum()
        #for j in range(0, decoder_outputs.shape[1]):
        #    w = pad_target_seqs[i][j]
        #    if w != padding_value:
        #        n = n+1
         #       loss += criterion(decoder_outputs[i][j].unsqueeze(0), pad_target_seqs[i][j].view(1)).sum()

    return loss/n

skip_training = False
trainset = data.JokeData()

trainloader = DataLoader(trainset, batch_size=20,
                        shuffle=True, num_workers=4, collate_fn=collate, pin_memory=True)
criterion = nn.NLLLoss(reduction='none')  
n_epochs = 20
teacher_forcing_ratio = 0.5

loss_total = 0 

encoder = Encoder(len(trainset.data.idx2word), 128).to(device)
decoder = Decoder(128, len(trainset.data.idx2word), SOS_token = trainset.SOS_token).to(device)

encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.005)
decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.005)

for epoch in range(n_epochs):
    running_loss = 0.0
    print_every = 20  # iterations
    for i, batch in enumerate(trainloader):
        pad_input_seqs, input_seq_lengths, pad_target_seqs, target_seq_lengths = batch
        batch_size = pad_input_seqs.size(1)
        pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)

        encoder_hidden = encoder.init_hidden(batch_size, device)
        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        # Encode input sequence
        _, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths, encoder_hidden)

        # Decode using target sequence for teacher forcing
        decoder_hidden = encoder_hidden
        teacher_forcing = True if random.random() < teacher_forcing_ratio else False
        decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs, teacher_forcing=teacher_forcing)

        # decoder_outputs is [max_seq_length, batch_size, output_dictionary_size]
        # pad_target_seqs in [max_seq_length, batch_size, 1]
        loss = compute_loss(decoder_outputs, pad_target_seqs, padding_value=0)
        loss.backward()

        encoder_optimizer.step()
        decoder_optimizer.step()
        print("done batch ", i, "/", len(trainloader))
        
        # print statistics
        running_loss += loss.item()
        if (i % print_every) == (print_every-1) or i == (len(trainset) // trainloader.batch_size):
            gc.collect()
            torch.cuda.empty_cache()
            print('[%d, %5d] loss: %.4f' % (epoch+1, i+1, running_loss/print_every))
            running_loss = 0.0

        if skip_training:
            break
    encoder_filename = '5_encoder_batch.pth'
    decoder_filename = '5_decoder_batch.pth'
    torch.save(encoder.state_dict(), encoder_filename)
    torch.save(decoder.state_dict(), decoder_filename)
    print('Model saved to %s, %s.' % (encoder_filename, decoder_filename))
    if skip_training:
        break

print('Finished Training')

# Save the model to disk, submit these files together with your notebook
encoder_filename = '5_encoder_batch.pth'
decoder_filename = '5_decoder_batch.pth'
if not skip_training:
    try:
        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()
        if do_save == 'yes':
            torch.save(encoder.state_dict(), encoder_filename)
            torch.save(decoder.state_dict(), decoder_filename)
            print('Model saved to %s, %s.' % (encoder_filename, decoder_filename))
        else:
            print('Model not saved.')
    except:
        raise Exception('The notebook should be run or validated with skip_training=True.')
else:
    hidden_size = 256
    encoder = Encoder(trainset.input_lang.n_words, hidden_size)
    encoder.load_state_dict(torch.load(encoder_filename, map_location=lambda storage, loc: storage))
    print('Encoder loaded from %s.' % encoder_filename)
    encoder = encoder.to(device)
    encoder.eval()

    decoder = Decoder(hidden_size, trainset.output_lang.n_words)
    decoder.load_state_dict(torch.load(decoder_filename, map_location=lambda storage, loc: storage))
    print('Decoder loaded from %s.' % decoder_filename)
    decoder = decoder.to(device)
    decoder.eval()



def evaluate(input_seq):
    with torch.no_grad():
        input_length = input_seq.size(0)
        batch_size = 1

        encoder_hidden = encoder.init_hidden(batch_size, device)
        input_seq = input_seq.view(-1, 1, 1).to(device)
        encoder_output, encoder_hidden = encoder(input_seq, [input_length], encoder_hidden)

        decoder_hidden = encoder_hidden
        decoder_outputs, decoder_hidden = decoder(decoder_hidden, pad_target_seqs=None, teacher_forcing=False)

        output_seq = []
        for t in range(decoder_outputs.size(0)):
            topv, topi = decoder_outputs[t].data.topk(1)
            output_seq.append(topi.item())
            if topi.item() == EOS_token:
                break

    return output_seq
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHywm1m8Kvs3",
        "colab_type": "text"
      },
      "source": [
        "First imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_XzKKITHOKa",
        "colab_type": "code",
        "outputId": "b55981a7-2605-4b4b-b479-24dc72b23ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "\n",
        "import time\n",
        "device = torch.device(\"cuda\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb7KnsUDK1G_",
        "colab_type": "text"
      },
      "source": [
        "Let's now load the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcoHCpzrK0IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.load_data()\n",
        "        self.N_TOKENS = len(self.idx2word)\n",
        "\n",
        "    def load_data(self):\n",
        "        lines = 0\n",
        "        self.idx2word.append('<E>')\n",
        "        self.word2idx['<E>'] = len(self.idx2word)-1\n",
        "        tokens = 0\n",
        "        with open('/content/drive/My Drive/shortjokes_noquote.txt', 'r') as data:\n",
        "          for line in data:\n",
        "              words = line.split() + ['<EOS>']\n",
        "              lines = lines + 1\n",
        "              tokens = tokens + len(words)\n",
        "              for word in words:\n",
        "                  if word not in self.word2idx:\n",
        "                      self.idx2word.append(word)\n",
        "                      self.word2idx[word] = len(self.idx2word)-1\n",
        "\n",
        "        self.inputs = []\n",
        "        self.ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        with open('/content/drive/My Drive/shortjokes_noquote.txt', 'r') as data:\n",
        "          for line in data:\n",
        "              words = line.split() + ['<EOS>']\n",
        "              ind = 0\n",
        "              input = torch.zeros([len(words)], dtype=torch.int64)\n",
        "              for word in words:\n",
        "                  input[ind] = self.word2idx[word]\n",
        "                  self.ids[token] = self.word2idx[word]\n",
        "                  token = token+1\n",
        "                  ind = ind+1\n",
        "              self.inputs.append(input)\n",
        "        data.close()\n",
        "        print(\"ids \", len(self.ids), \" tokens \", len(self.idx2word))\n",
        "        print(len(input), \" tensors.\")\n",
        "\n",
        "class JokeData(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = Data()\n",
        "        self.EOS_token = self.data.word2idx['<EOS>']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.inputs[idx]\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwFT37T8ch82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnCIKIokiRkx",
        "colab_type": "text"
      },
      "source": [
        "Let's now define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy4P0vKciQ3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordRNN(nn.Module):\n",
        "    def __init__(self, ntoken, ninp=200, nhid = 128, nlayers = 1, dropout=0.2):\n",
        "        super(WordRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.embedding(input)\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRWHLW1yqAP3",
        "colab_type": "code",
        "outputId": "b0da4b45-3c89-4283-8b73-1a6a6b735695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len_tmp = min(seq_len, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len_tmp]\n",
        "    target = source[i+1:i+1+seq_len_tmp].view(-1)\n",
        "    return data, target\n",
        "\n",
        "trainset = JokeData()\n",
        "batch_size = 64\n",
        "train_data = batchify(trainset.data.ids, batch_size)\n",
        "seq_len = 42\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ids  4298259  tokens  228942\n",
            "20  tensors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwlRPTUlwd4C",
        "colab_type": "code",
        "outputId": "a5ef83f8-4fc0-4815-a3d5-0e8df4a109ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model = WordRNN(ntoken=len(trainset.data.idx2word)).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdHdmFknXCnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/lol.weight\"))\n",
        "model.lstm.flatten_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJOHkmcF0aD3",
        "colab_type": "text"
      },
      "source": [
        "Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBYuAsNQ6isY",
        "colab_type": "code",
        "outputId": "dd994112-e474-4a34-92e7-7acc052a477d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 12\n",
        "lr = 20\n",
        "learning_rate = 1e-4\n",
        "while epochs > 0:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    print(\"aloitetaan epoch\")\n",
        "    epochs = epochs-1\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(trainset.data.idx2word)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    seq_len = 35+random.randint(-10, 10)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        #print(data, \" data\", data.shape)\n",
        "        #print(targets, \"targets \", targets.shape)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        #for p in model.parameters():\n",
        "        #    p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0 and batch > 0:\n",
        "            cur_loss = total_loss / 100\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\"cur loss \", cur_loss)\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aloitetaan epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-78386c690a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 11.17 GiB total capacity; 8.94 GiB already allocated; 421.94 MiB free; 762.06 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MweNXmM1Y4b2",
        "colab_type": "code",
        "outputId": "7a159424-8b34-4b74-8f84-ea24f9459316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/My Drive/lollower.weight\")\n",
        "\n",
        "\n",
        "with open(\"malli2.lol\", 'wb') as f:\n",
        "                torch.save(model, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type WordRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn_eL_le0Y-A",
        "colab_type": "text"
      },
      "source": [
        "Normal sampling and top-k sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZH5vCfvrA1d",
        "colab_type": "code",
        "outputId": "1561de34-bce7-46c7-c504-90fb43e6909e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "words = 1000\n",
        "temp = 1.0\n",
        "model.eval()\n",
        "hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        input.fill_(word_idx)\n",
        "        word = trainset.data.idx2word[word_idx]\n",
        "\n",
        "        print(word,' ', end = ('\\n' if word == \"<EOS>\" else ' '))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to   show   me   this   Trump   jokes.   <EOS>  \n",
            "I   think   I   have   the   right   words   in   your   future.   Is   it   not   child   edited:   Damn   enough   to   delete   it   or   maybe   this   earth   looks   like   cake   What   don't   you   get?   <EOS>  \n",
            "A   female   frog   falls   by   a   trash   attack   on   a   two   cats   eating   apples.   <EOS>  \n",
            "The   forgot   I   bad   women.   Her   name   is   flipped   of   kissing   84   Really   drunk   and   the   next   door   and   stop   taking   a   nap.   <EOS>  \n",
            "So   a   day...   quietly   trying   to   make   you   PC   from   before   I   and   the   other   hand,   is.   <EOS>  \n",
            "At   my   Facebook   start   the   desk   &   I   have   to   cut   the   toilet   out   of   me   and   a   cheap   it   <EOS>  \n",
            "Everyone   is   going   to   meet   someone   with   really   good   shit,   but   they   don't   call   it   the   soooo   better.   <EOS>  \n",
            "When   I   am   happy   to   avoid   watching   just   food   sounds   like   you   think   that   how   do   that.   <EOS>  \n",
            "My   discovered   I   only   speak   list   of   women   than   Russian   believe   the   7   guy   curves.   <EOS>  \n",
            "I   recently   farted   at   the   world's   most   popular   dressing   When   he   asks   me   in   public   Laugh   our   Clinton   play   crying   pour   him   of   a   red   guy   sky   drowns   down   down   his   face.   <EOS>  \n",
            "Sure   on   your   boss,   new   spot   at   a   shop   too   long   for   some   week.   I   said,   \"\"That's   Hi   she   ...   do   we   do   his   shirt   &   I   have   to   have   to   do   this   one   coming.   <EOS>  \n",
            "Me:   When   she   knows   I   should   really   was   a   I'm   home.\"\"   I   was   glad   I   heard   how   I   died?   That's   the   3rd   rule   of   not?   <EOS>  \n",
            "My   friend   asked   me   if   I   were   into   this   day.   Yes,   I   would   always   did   it.   He   said   Chinese   baby   were   some   the   experts   at   the   wrong   every   room.   :(   <EOS>  \n",
            "Just   went   used   to   have   this   bag   of   a   hot   dog   There   are   no   hair   to   defend   my   head   at   shoplifting   before   making   his   open.   <EOS>  \n",
            "I   think   my   favorite   year   if   we   were   out   of   my   kale   in   the   nevermind   this   one:   I   hope   the   houses   can't   stand   together.   <EOS>  \n",
            "It   comes   in   school   and,   well,   there   will   be   my   iPhone   5   stories   it's   old   enough   to   ride   its   shot   \"\"How   travel   should   eat   your   right   leg   to   do   his   leg?   <EOS>  \n",
            "Let's   get   something   drunk   phrase   lamps.   By   first,   but   they're   screaming,   british   asshole.   <EOS>  \n",
            "You   want   to   borrow   the   cat's   main   product   in   your   but   never   lost   your   pants..   it   drives   her   clothes   off   it.   <EOS>  \n",
            "What   did   the   women!!!   say   when   he   mixed   up   his   \"\"OK   Patty   stool   <EOS>  \n",
            "There's   no   eye   eye   into   a   poll   of   the   following   great!   Although   not   it   a   Without   to   Murray.   <EOS>  \n",
            "Japan   and   you   never   turned   into   a   doesn't   you   wanna   hear   a   joke   about   it   I   hear   was   the   worst   punchline   <EOS>  \n",
            "Have   you   ever   heard   the   one   about   the   New   Year   engine?   No?   Really?   neither   gets   it?   <EOS>  \n",
            "What   do   you   call   some   man   that   drives   carrying   cancer   into   his   half   Day?   Wonderwall   mini-me   <EOS>  \n",
            "man   who   doesn't   wish   he   had   an   emergency   relationship   with   reddit   Ulysses   have   no   buccaneer.   <EOS>  \n",
            "I'm   meant   to   old   wish   you   had   a   set   of   snare   \"\"What   did   that   be   that   home   in   my   car   crash.   \"\"Come   on   jokes   from   the   at   your   *\"\"But   I   can't   believe   I'm   here   all   night   and   walks   into   this   day's   movement.   <EOS>  \n",
            "I'm   going   to   spend   pushing   the   bed   so   cold   all   day   in   much   the   economy   Oxidants   most   powerful   you.   <EOS>  \n",
            "What   does   Wonder   say   to   the   other?   A   Chopin   Haagen-Dazs   bastard.   <EOS>  \n",
            "What   does   a   major   learn   about   talkin   for   pregnancy   tests   about   being   ridiculous.   <EOS>  \n",
            "I   just   started   home   station,   of   my   Australian   puns   Because   it   takes   cheesy   for   the   Burnham   <EOS>  \n",
            "Whats   the   difference   between   a   politician   and   a   man   who   can   play   with   the   punching   stuff   in   the   last   night?   <EOS>  \n",
            "WHEN   I   heard   the   last   time   he   could   shoot   us   today   He   finally   had   2   fucks   me   <EOS>  \n",
            "\"\"Our   wedding   details   of   to   Shit?   Baltimore   TheFineBros.   <EOS>  \n",
            "I   always   been   eating   chlorine   by   Pizza   We're   on   TV.   Oh   god   Im   too   \"\"You   need   to   have   a   kid   physical   whine   about   it,   right?   <EOS>  \n",
            "What   can   do   girls   make   the   grass?   speak   bill   You   look   alot   of   fun   Brain:   prison??   Do   You   want   many   crackle,   We   say,   it   was   stuck   on   the   top   shelf.   <EOS>  \n",
            "What   did   the   Jewish   girl   say   to   the   job.   Pick   Michael   \"\"Damn   remembers   being   a   \"\"Conjunctivitis.com   (A   Asperger's   falls   out   of   my   *I   swigged   from   a   bottle   <EOS>  \n",
            "I've   do   nothing   people   with   my   burrito.   ..I   jokes   are   quiet   but   when   that   happens   to   GENIUS:   where   to   be   posters   to   think   anyone   makes   it   emerges]   6yo:   <EOS>  \n",
            "What   should   anyone   A   cow   and   a   Jew?   I   use   barks   in   one   was   on   the   face   on   their   mouth   <EOS>  \n",
            "What   is   a   Jihadist   of   the   snow   female   but   ends   with   potato.   The   government   it's   night.   <EOS>  \n",
            "*goes   up   in   hospital   go   home   to   wish   and   40   ate   her   own   with   wife   so   I   am   going   to   racist   you   missed   that   one   side   <EOS>  \n",
            "What   do   you   call   a   cheap   circumcision?   A   rip   it   off!   <EOS>  \n",
            "To   which   a   duck   joke   21/12/2012.   a   baguette   at   an   elevator...   He   will   make   me   Jack   off.   <EOS>  \n",
            "Why   do   I   keep   up   with   other   people?   We   want   to   use   the   First   they   can   see   if   I   have   had   an   elephant?   <EOS>  \n",
            "My   cat   keeps   saying   \"\"Yeah   not\"\"   with   his   brother   the   karate   stool.   <EOS>  \n",
            "It's   ironic   how   the   six   humped   you   are,   a   second   in   the   hot   hockey   team   and   I   needed   time   to   bake   eye   contact   as   I   was   already   into   his   nose   <EOS>  \n",
            "My   wife   had   his   job   for   a   new   keyboard   this   morning.   He   just   wanted   to   use   a   hammer   to   the   people?   <EOS>  \n",
            "\"\"What's   the   difference   between   silly   boyfriend   and   a   banana   Paul   sleeps   with   the   breast   implants   that   plays   it   with   me.   <EOS>  \n",
            "I   at   my   ocean   feel   about   "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL83mamdlRUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = 1000\n",
        "temp = 1.0\n",
        "model.eval()\n",
        "hidden = model.init_hidden(1)\n",
        "topk = 40\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        weights, ind = torch.topk(output.squeeze(), topk)\n",
        "        word_weights = weights.div(temp).exp().cpu()\n",
        "        word_idx = ind[torch.multinomial(word_weights, 1)[0]]\n",
        "        input.fill_(word_idx)\n",
        "        word = trainset.data.idx2word[word_idx]\n",
        "\n",
        "        print(word,' ', end = ('\\n' if word == \"<EOS>\" else ' '))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
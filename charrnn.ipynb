{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "charrnn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Btjo6rpkDQa",
        "colab_type": "code",
        "outputId": "2eb71444-1406-471b-d2ec-66f37914059d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import files\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "\n",
        "import time\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muBvri18kLYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data(object):\n",
        "    def __init__(self):\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = []\n",
        "        \n",
        "        self.EOS = '\\n'\n",
        "        self.padding = '\\0'\n",
        "        self.load_data()\n",
        "        self.N_TOKENS = len(self.idx2char)\n",
        "\n",
        "    def load_data(self):\n",
        "        lines = 0\n",
        "        self.idx2char.append(self.padding)\n",
        "        self.char2idx[self.padding] = len(self.idx2char)-1\n",
        "        tokens = 0\n",
        "        with open('/content/drive/My Drive/shortjokes_noquote.txt', 'r') as data:\n",
        "          for line in data:\n",
        "              line = line + self.EOS\n",
        "              lines = lines + 1\n",
        "              tokens = tokens + len(line)\n",
        "              for char in line:\n",
        "                  if char not in self.char2idx:\n",
        "                      self.idx2char.append(char)\n",
        "                      self.char2idx[char] = len(self.idx2char)-1\n",
        "\n",
        "        self.inputs = []\n",
        "        self.ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        with open('/content/drive/My Drive/shortjokes_noquote.txt', 'r') as data:\n",
        "          for line in data:\n",
        "              line = line + self.EOS\n",
        "              ind = 0\n",
        "              input = torch.zeros([len(line)], dtype=torch.int64)\n",
        "              for char in line:\n",
        "                  input[ind] = self.char2idx[char]\n",
        "                  self.ids[token] = self.char2idx[char]\n",
        "                  token = token+1\n",
        "                  ind = ind+1\n",
        "              self.inputs.append(input)\n",
        "        data.close()\n",
        "        print(\"ids \", len(self.ids), \" tokens \", len(self.idx2char))\n",
        "        print(len(input), \" tensors.\")\n",
        "\n",
        "class JokeData(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = Data()\n",
        "        self.EOS_token = self.data.char2idx[self.data.EOS]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.inputs[idx]\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6dpLyk3FFEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordRNN(nn.Module):\n",
        "    def __init__(self, ntoken, ninp=200, nhid = 128, nlayers = 1, dropout=0.1):\n",
        "        super(WordRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.embedding(input)\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyhd72bXFF2t",
        "colab_type": "code",
        "outputId": "fdbc3b41-ada8-48fd-c2a7-b172556a8c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len_tmp = min(seq_len, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len_tmp]\n",
        "    target = source[i+1:i+1+seq_len_tmp].view(-1)\n",
        "    return data, target\n",
        "\n",
        "trainset = JokeData()\n",
        "batch_size = 64\n",
        "train_data = batchify(trainset.data.ids, batch_size)\n",
        "seq_len = 120"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ids  22111965  tokens  99\n",
            "104  tensors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnPtmac-FKSY",
        "colab_type": "code",
        "outputId": "dde505d0-6bc7-45de-c35f-b93f6553d154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model = WordRNN(ntoken=len(trainset.data.idx2char)).to(device)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5pHJGr_FObH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/lolchar.weight\"))\n",
        "model.lstm.flatten_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRyuNu30RVV",
        "colab_type": "text"
      },
      "source": [
        "Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hma7ZgBFTcd",
        "colab_type": "code",
        "outputId": "e95cc911-ebd7-43ca-e36b-1cad94c77a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2976
        }
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 30\n",
        "lr = 20\n",
        "learning_rate = 1e-4\n",
        "while epochs > 0:\n",
        "    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "    print(\"aloitetaan epoch\")\n",
        "    epochs = epochs-1\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(trainset.data.idx2char)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    seq_len = 160+random.randint(-20, 20)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        #print(data, \" data\", data.shape)\n",
        "        #print(targets, \"targets \", targets.shape)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        #for p in model.parameters():\n",
        "        #    p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0 and batch > 0:\n",
        "            cur_loss = total_loss / 100\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\"cur loss \", cur_loss)\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aloitetaan epoch\n",
            "cur loss  4.627683458328247\n",
            "cur loss  4.564017457962036\n",
            "cur loss  4.549938397407532\n",
            "cur loss  4.535290231704712\n",
            "cur loss  4.519381713867188\n",
            "cur loss  4.499804530143738\n",
            "cur loss  4.474087252616882\n",
            "cur loss  4.434834461212159\n",
            "cur loss  4.34435293674469\n",
            "cur loss  3.8905296182632445\n",
            "cur loss  3.6699304294586184\n",
            "cur loss  3.5860158014297485\n",
            "cur loss  3.5305615401268007\n",
            "cur loss  3.4950987339019775\n",
            "cur loss  3.4615744280815126\n",
            "cur loss  3.438730595111847\n",
            "cur loss  3.421661925315857\n",
            "cur loss  3.4047255730628967\n",
            "cur loss  3.390179512500763\n",
            "cur loss  3.3789824271202087\n",
            "cur loss  3.3701874589920044\n",
            "cur loss  3.359983150959015\n",
            "cur loss  3.354071419239044\n",
            "aloitetaan epoch\n",
            "cur loss  3.3481018948554992\n",
            "cur loss  3.290704746246338\n",
            "cur loss  3.286099274158478\n",
            "cur loss  3.27209401845932\n",
            "cur loss  3.2740192866325377\n",
            "cur loss  3.2655307245254517\n",
            "cur loss  3.261631441116333\n",
            "cur loss  3.2582183933258055\n",
            "cur loss  3.2574877023696898\n",
            "cur loss  3.2581403398513795\n",
            "cur loss  3.2522103548049928\n",
            "cur loss  3.254225342273712\n",
            "cur loss  3.2532073354721067\n",
            "cur loss  3.2511417770385744\n",
            "cur loss  3.250617272853851\n",
            "cur loss  3.249102714061737\n",
            "cur loss  3.2475702500343324\n",
            "cur loss  3.2452973747253417\n",
            "cur loss  3.243655278682709\n",
            "cur loss  3.246137022972107\n",
            "cur loss  3.2431772923469544\n",
            "cur loss  3.239469151496887\n",
            "cur loss  3.243593728542328\n",
            "aloitetaan epoch\n",
            "cur loss  3.2701036810874937\n",
            "cur loss  3.2311458516120912\n",
            "cur loss  3.2327044653892516\n",
            "cur loss  3.225526518821716\n",
            "cur loss  3.226029622554779\n",
            "cur loss  3.221597752571106\n",
            "cur loss  3.216634380817413\n",
            "cur loss  3.215295751094818\n",
            "cur loss  3.2156713438034057\n",
            "cur loss  3.2133418560028075\n",
            "cur loss  3.21361049413681\n",
            "cur loss  3.2082499194145204\n",
            "cur loss  3.2112195682525635\n",
            "cur loss  3.2066562175750732\n",
            "cur loss  3.2048650336265565\n",
            "cur loss  3.2016424679756166\n",
            "cur loss  3.1996014642715456\n",
            "cur loss  3.198642888069153\n",
            "cur loss  3.1964338850975036\n",
            "cur loss  3.1934970355033876\n",
            "cur loss  3.194778425693512\n",
            "aloitetaan epoch\n",
            "cur loss  3.217978951931\n",
            "cur loss  3.176555118560791\n",
            "cur loss  3.1757079792022704\n",
            "cur loss  3.166181693077087\n",
            "cur loss  3.1672556376457215\n",
            "cur loss  3.159395470619202\n",
            "cur loss  3.155748734474182\n",
            "cur loss  3.1492840170860292\n",
            "cur loss  3.1484567427635195\n",
            "cur loss  3.145616629123688\n",
            "cur loss  3.1435962247848512\n",
            "cur loss  3.1415734767913817\n",
            "cur loss  3.1387971162796022\n",
            "cur loss  3.136454825401306\n",
            "cur loss  3.1347951674461365\n",
            "cur loss  3.130625836849213\n",
            "cur loss  3.1269841098785403\n",
            "cur loss  3.126950833797455\n",
            "cur loss  3.122355914115906\n",
            "cur loss  3.120105984210968\n",
            "cur loss  3.1191288518905638\n",
            "cur loss  3.1170741748809814\n",
            "cur loss  3.112732253074646\n",
            "cur loss  3.113541748523712\n",
            "aloitetaan epoch\n",
            "cur loss  3.132958347797394\n",
            "cur loss  3.0900631189346313\n",
            "cur loss  3.0872608184814454\n",
            "cur loss  3.0753618693351745\n",
            "cur loss  3.0754095244407655\n",
            "cur loss  3.0669101810455324\n",
            "cur loss  3.0621171045303344\n",
            "cur loss  3.0553682708740233\n",
            "cur loss  3.054620022773743\n",
            "cur loss  3.05181991815567\n",
            "cur loss  3.0485019516944885\n",
            "cur loss  3.0463898897171022\n",
            "cur loss  3.0424462223052977\n",
            "cur loss  3.0396596026420593\n",
            "cur loss  3.035619788169861\n",
            "cur loss  3.036162610054016\n",
            "cur loss  3.0307077050209044\n",
            "cur loss  3.0288496661186217\n",
            "cur loss  3.023330433368683\n",
            "cur loss  3.0231847071647646\n",
            "cur loss  3.0202347326278685\n",
            "cur loss  3.016100549697876\n",
            "cur loss  3.018234496116638\n",
            "aloitetaan epoch\n",
            "cur loss  3.0328450751304628\n",
            "cur loss  2.99008535861969\n",
            "cur loss  2.986380035877228\n",
            "cur loss  2.974394452571869\n",
            "cur loss  2.9721143531799314\n",
            "cur loss  2.966209526062012\n",
            "cur loss  2.957925159931183\n",
            "cur loss  2.953955225944519\n",
            "cur loss  2.9523611187934877\n",
            "cur loss  2.9485982418060304\n",
            "cur loss  2.9455796098709106\n",
            "cur loss  2.9421735882759092\n",
            "cur loss  2.94098375082016\n",
            "cur loss  2.9354274892807006\n",
            "cur loss  2.9340125846862795\n",
            "cur loss  2.9295697021484375\n",
            "cur loss  2.9265861678123475\n",
            "cur loss  2.9240824127197267\n",
            "cur loss  2.9226790022850038\n",
            "cur loss  2.918369550704956\n",
            "cur loss  2.9185613489151\n",
            "aloitetaan epoch\n",
            "cur loss  2.930781087875366\n",
            "cur loss  2.8918808579444883\n",
            "cur loss  2.8828371143341065\n",
            "cur loss  2.877740023136139\n",
            "cur loss  2.8692721796035765\n",
            "cur loss  2.8638081336021424\n",
            "cur loss  2.85828227519989\n",
            "cur loss  2.8572666025161744\n",
            "cur loss  2.8526334404945373\n",
            "cur loss  2.850648753643036\n",
            "cur loss  2.846033203601837\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-9c5988d7fc68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08xQ3XOgI6ma",
        "colab_type": "code",
        "outputId": "112605d2-e3be-4523-8377-f3474b391a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/My Drive/lolchar4.weight\")\n",
        "\n",
        "\n",
        "with open(\"mallichar.lol\", 'wb') as f:\n",
        "                torch.save(model, f)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type WordRNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSHBZntn0Vnj",
        "colab_type": "text"
      },
      "source": [
        "Normal sampling and top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuipi3k6JAOE",
        "colab_type": "code",
        "outputId": "a89a900b-6b22-49bf-8aeb-fd9f2b95bd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "words = 1000\n",
        "temp = 1.0\n",
        "hidden = model.init_hidden(1)\n",
        "model.eval()\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        word_weights = output.squeeze().div(temp).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        input.fill_(word_idx)\n",
        "        word = trainset.data.idx2char[word_idx]\n",
        "\n",
        "        print(word, end = '')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/~rrh7H oRrRe qh: jj'es gr>at iso . thnrad an we rmehog hcpe oso btr\n",
            ",e wsiteiK\n",
            "gI insts to.V aanroa binnu tepse  teinsr iatn ougwer meg tellytr n\n",
            "Hut ca!un a orssid ta'oh cdroMa'in vee isin aodt lea tet? jSWs wah tohsl haodr de aLeo..s ss twege en Maepsug l epe tua son?ve\n",
            "oirdes\n",
            "u\n",
            "Dy ta des bd gesn asat EoL\n",
            "\n",
            "Icahs, rsa aaa.n toaut  Le  onf boob  tb\".MR.rio'T fce d\"rherd teic nho t\"re ikdliiamy  ?ovcu iy doigxma aanne\n",
            " yre.W.\"e dbfo too etii.o pok \n",
            "hks Fd'\n",
            " ae woe tign bmootit Aisr yn Fih toIhio ?\n",
            "\n",
            "WhpteT ere heiuhm.a Ocbsus ye 'in hem hsy argip,e' k iba conat weha a mmeepdt\" OsGN !.Nts mienJt\n",
            ".haes atka the iWr hhkeir rb ne utk un dtt tiamilFxp1MWfy\n",
            "\"cety d ey , narstA an t\n",
            "eunnnr geo sor/ af teo wlad stre wed tcar foulnt at yosmsnog\n",
            " yti Xe. fatf.Th. Ihhe Pnr  omar llysR d ralliucg ma etce aierhevg sthwu leirs  aaanou!s/olg ocosn IrSss whr wcrnM,?hy fha Aa lee,o'ev\" Wede wol\n",
            "natv ofd tsores.\n",
            "?':pEe t aw cre htnjo tiweia'g Y\n",
            "ins. du ifna tsot iwmTn a-ied, I o!u ho boaod \"oi ssr ko<yt "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPD3R-XF0NIP",
        "colab_type": "code",
        "outputId": "53153063-3f77-497f-b370-ced45345e84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "words = 1000\n",
        "temp = 1.0\n",
        "model.eval()\n",
        "hidden = model.init_hidden(1)\n",
        "topk = 40\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        weights, ind = torch.topk(output.squeeze(), topk)\n",
        "        word_weights = weights.div(temp).exp().cpu()\n",
        "        word_idx = ind[torch.multinomial(word_weights, 1)[0]]\n",
        "        input.fill_(word_idx)\n",
        "        word = trainset.data.idx2char[word_idx]\n",
        "\n",
        "        print(word, end = '')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gazvof\n",
            "\n",
            " al\" sl.\n",
            "SIhy. df lcwea\"\n",
            "\n",
            " ohfI a icbrebcum pl aoute dred aare sityd tk thte sdrle nre hne.edt me  aanest wet wl nord a,lesf tnte.\n",
            "\"fvete'ntnn yon tocbocersd i'ite s hoe tjt ucathre elne iltlviobpeunmd bhaaigi ns ahhg \n",
            "ocis booy dri lhede ie de as s!ydwfd .\"Mudtdnr thebe cdao pent tetme? neft go bvpect nandyvI \n",
            "onox whe awrded wechas achutc fruite taeni? lre hecs anocte sH\n",
            "t\"l yothees peiro tholbIrd ?fTanguy dwancif tp a\n",
            "ki idd acky piam aalyr!\n",
            "Wse\" cafin mofo s faerea dooho ea  er aa eam?: \n",
            "oi. whgie chanisg hol s agh ton tlhouure\n",
            "tr sW\n",
            " wih dl doye \"ho mahiteH:i sida we auah,? rhnf'amertd' Ih?r dogt\n",
            "icBha ato oooregot.Meiu ao Iwo tbwe cosold rf\n",
            "s lo arfis'nsd si s meadpe rhtpimise riglt pacnag orthehtibs cleg nnt'. Io nsH mri.f ren, a\n",
            "cweaat su ts wheeIr \n",
            "ycs aeshiws Iaet degae y.g ,is e lfils kt lemhe w ncerrn bonnist hhu nmaf\"  wshia\n",
            "Tt onice\" udi thbllooAA ohs aetl.l?r\"hk nao ?\n",
            "lcoutnImem iats.h yd e tannoilh\n",
            "o'ts eis di ie ovtc,s miru oo ?eh hru s.uw hns afos alr Is go ns"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}